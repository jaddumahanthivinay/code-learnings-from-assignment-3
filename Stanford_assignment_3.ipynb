{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision"
      ],
      "metadata": {
        "id": "jQESOCJ4aU3c"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "2cjBtsMKbSmb"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxdziBRrbl5K",
        "outputId": "27e7580c-1e53-47a1-aee1-ecf67b34acbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=f2b2cc299cfc370a68996b48e388fb91b4e58e742a8edc55f8c02a5e2f115ccb\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt\n",
            "Successfully installed docopt-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from docopt import docopt"
      ],
      "metadata": {
        "id": "C9doU9_1b7V4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2020-2021: Homework 3\n",
        "parser_transitions.py: Algorithms for completing partial parsess.\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "Haoshen Hong <haoshen@stanford.edu>\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "\n",
        "class PartialParse(object):\n",
        "    def __init__(self, sentence):\n",
        "        \"\"\"Initializes this partial parse.\n",
        "\n",
        "        @param sentence (list of str): The sentence to be parsed as a list of words.\n",
        "                                        Your code should not modify the sentence.\n",
        "        \"\"\"\n",
        "        # The sentence being parsed is kept for bookkeeping purposes. Do NOT alter it in your code.\n",
        "        self.sentence = sentence\n",
        "\n",
        "        ### YOUR CODE HERE (3 Lines)\n",
        "        ### Your code should initialize the following fields:\n",
        "        ###     self.stack: The current stack represented as a list with the top of the stack as the\n",
        "        ###                 last element of the list.\n",
        "        ###     self.buffer: The current buffer represented as a list with the first item on the\n",
        "        ###                  buffer as the first item of the list\n",
        "        ###     self.dependencies: The list of dependencies produced so far. Represented as a list of\n",
        "        ###             tuples where each tuple is of the form (head, dependent).\n",
        "        ###             Order for this list doesn't matter.\n",
        "        ###\n",
        "        ### Note: The root token should be represented with the string \"ROOT\"\n",
        "        ### Note: If you need to use the sentence object to initialize anything, make sure to not directly \n",
        "        ###       reference the sentence object.  That is, remember to NOT modify the sentence object. \n",
        "        self.stack = ['ROOT']\n",
        "        self.buffer = self.sentence.copy()\n",
        "        self.dependencies = []\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "\n",
        "    def parse_step(self, transition):\n",
        "        \"\"\"Performs a single parse step by applying the given transition to this partial parse\n",
        "\n",
        "        @param transition (str): A string that equals \"S\", \"LA\", or \"RA\" representing the shift,\n",
        "                                left-arc, and right-arc transitions. You can assume the provided\n",
        "                                transition is a legal transition.\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE (~7-12 Lines)\n",
        "        ### TODO:\n",
        "        ###     Implement a single parsing step, i.e. the logic for the following as\n",
        "        ###     described in the pdf handout:\n",
        "        ###         1. Shift\n",
        "        ###         2. Left Arc\n",
        "        ###         3. Right Arc\n",
        "        if transition == 'S':\n",
        "            # remove the first word from the buffer and pushes it onto the stack\n",
        "            self.stack.append(self.buffer.pop(0))\n",
        "        elif transition == 'LA':\n",
        "            # mark the second item on the stack as a dependent of the first item\n",
        "            self.dependencies.append((self.stack[-1], self.stack[-2]))\n",
        "            # remove the second item from the stack\n",
        "            self.stack.pop(-2)\n",
        "        elif transition == 'RA':\n",
        "            # mark the first item on the stack as a dependent of the second item\n",
        "            self.dependencies.append((self.stack[-2], self.stack[-1]))\n",
        "            # remove the first item from the stack\n",
        "            self.stack.pop()\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "\n",
        "    def parse(self, transitions):\n",
        "        \"\"\"Applies the provided transitions to this PartialParse\n",
        "\n",
        "        @param transitions (list of str): The list of transitions in the order they should be applied\n",
        "\n",
        "        @return dependencies (list of string tuples): The list of dependencies produced when\n",
        "                                                        parsing the sentence. Represented as a list of\n",
        "                                                        tuples where each tuple is of the form (head, dependent).\n",
        "        \"\"\"\n",
        "        for transition in transitions:\n",
        "            self.parse_step(transition)\n",
        "        return self.dependencies\n",
        "\n",
        "\n",
        "def minibatch_parse(sentences, model, batch_size):\n",
        "    \"\"\"Parses a list of sentences in minibatches using a model.\n",
        "\n",
        "    @param sentences (list of list of str): A list of sentences to be parsed\n",
        "                                            (each sentence is a list of words and each word is of type string)\n",
        "    @param model (ParserModel): The model that makes parsing decisions. It is assumed to have a function\n",
        "                                model.predict(partial_parses) that takes in a list of PartialParses as input and\n",
        "                                returns a list of transitions predicted for each parse. That is, after calling\n",
        "                                    transitions = model.predict(partial_parses)\n",
        "                                transitions[i] will be the next transition to apply to partial_parses[i].\n",
        "    @param batch_size (int): The number of PartialParses to include in each minibatch\n",
        "\n",
        "\n",
        "    @return dependencies (list of dependency lists): A list where each element is the dependencies\n",
        "                                                    list for a parsed sentence. Ordering should be the\n",
        "                                                    same as in sentences (i.e., dependencies[i] should\n",
        "                                                    contain the parse for sentences[i]).\n",
        "    \"\"\"\n",
        "    dependencies = []\n",
        "\n",
        "    ### YOUR CODE HERE (~8-10 Lines)\n",
        "    ### TODO:\n",
        "    ###     Implement the minibatch parse algorithm.  Note that the pseudocode for this algorithm is given in the pdf handout.\n",
        "    ###\n",
        "    ###     Note: A shallow copy (as denoted in the PDF) can be made with the \"=\" sign in python, e.g.\n",
        "    ###                 unfinished_parses = partial_parses[:].\n",
        "    ###             Here `unfinished_parses` is a shallow copy of `partial_parses`.\n",
        "    ###             In Python, a shallow copied list like `unfinished_parses` does not contain new instances\n",
        "    ###             of the object stored in `partial_parses`. Rather both lists refer to the same objects.\n",
        "    ###             In our case, `partial_parses` contains a list of partial parses. `unfinished_parses`\n",
        "    ###             contains references to the same objects. Thus, you should NOT use the `del` operator\n",
        "    ###             to remove objects from the `unfinished_parses` list. This will free the underlying memory that\n",
        "    ###             is being accessed by `partial_parses` and may cause your code to crash.\n",
        "    for i in sentences:\n",
        "      partial_parses = PartialParse(i)\n",
        "    unfinished_parses = partial_parses[:]\n",
        "\n",
        "    while len(unfinished_parses)>0:\n",
        "      first_batch_size = unfinished_parses[:batch_size]\n",
        "      batch_transition = model.predict(first_batch_size)\n",
        "      for j, k in zip(first_batch_size, batch_transition):\n",
        "        j.parse_step(k)\n",
        "        if len(j.buffer)==0 and len(j.stack)==1:\n",
        "          unfinished_parses.remove(j)\n",
        "\n",
        "    for l in partial_parses:\n",
        "      dependencies = l.dependencies\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    return dependencies\n",
        "\n",
        "\n",
        "def test_step(name, transition, stack, buf, deps,\n",
        "              ex_stack, ex_buf, ex_deps):\n",
        "    \"\"\"Tests that a single parse step returns the expected output\"\"\"\n",
        "    pp = PartialParse([])\n",
        "    pp.stack, pp.buffer, pp.dependencies = stack, buf, deps\n",
        "\n",
        "    pp.parse_step(transition)\n",
        "    stack, buf, deps = (tuple(pp.stack), tuple(pp.buffer), tuple(sorted(pp.dependencies)))\n",
        "    assert stack == ex_stack, \\\n",
        "        \"{:} test resulted in stack {:}, expected {:}\".format(name, stack, ex_stack)\n",
        "    assert buf == ex_buf, \\\n",
        "        \"{:} test resulted in buffer {:}, expected {:}\".format(name, buf, ex_buf)\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "    print(\"{:} test passed!\".format(name))\n",
        "\n",
        "\n",
        "def test_parse_step():\n",
        "    \"\"\"Simple tests for the PartialParse.parse_step function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    test_step(\"SHIFT\", \"S\", [\"ROOT\", \"the\"], [\"cat\", \"sat\"], [],\n",
        "              (\"ROOT\", \"the\", \"cat\"), (\"sat\",), ())\n",
        "    test_step(\"LEFT-ARC\", \"LA\", [\"ROOT\", \"the\", \"cat\"], [\"sat\"], [],\n",
        "              (\"ROOT\", \"cat\",), (\"sat\",), ((\"cat\", \"the\"),))\n",
        "    test_step(\"RIGHT-ARC\", \"RA\", [\"ROOT\", \"run\", \"fast\"], [], [],\n",
        "              (\"ROOT\", \"run\",), (), ((\"run\", \"fast\"),))\n",
        "\n",
        "\n",
        "def test_parse():\n",
        "    \"\"\"Simple tests for the PartialParse.parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "    sentence = [\"parse\", \"this\", \"sentence\"]\n",
        "    dependencies = PartialParse(sentence).parse([\"S\", \"S\", \"S\", \"LA\", \"RA\", \"RA\"])\n",
        "    dependencies = tuple(sorted(dependencies))\n",
        "    expected = (('ROOT', 'parse'), ('parse', 'sentence'), ('sentence', 'this'))\n",
        "    assert dependencies == expected,  \\\n",
        "        \"parse test resulted in dependencies {:}, expected {:}\".format(dependencies, expected)\n",
        "    assert tuple(sentence) == (\"parse\", \"this\", \"sentence\"), \\\n",
        "        \"parse test failed: the input sentence should not be modified\"\n",
        "    print(\"parse test passed!\")\n",
        "\n",
        "\n",
        "class DummyModel(object):\n",
        "    \"\"\"Dummy model for testing the minibatch_parse function\n",
        "    \"\"\"\n",
        "    def __init__(self, mode = \"unidirectional\"):\n",
        "        self.mode = mode\n",
        "\n",
        "    def predict(self, partial_parses):\n",
        "        if self.mode == \"unidirectional\":\n",
        "            return self.unidirectional_predict(partial_parses)\n",
        "        elif self.mode == \"interleave\":\n",
        "            return self.interleave_predict(partial_parses)\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "    def unidirectional_predict(self, partial_parses):\n",
        "        \"\"\"First shifts everything onto the stack and then does exclusively right arcs if the first word of\n",
        "        the sentence is \"right\", \"left\" if otherwise.\n",
        "        \"\"\"\n",
        "        return [(\"RA\" if pp.stack[1] is \"right\" else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
        "                for pp in partial_parses]\n",
        "\n",
        "    def interleave_predict(self, partial_parses):\n",
        "        \"\"\"First shifts everything onto the stack and then interleaves \"right\" and \"left\".\n",
        "        \"\"\"\n",
        "        return [(\"RA\" if len(pp.stack) % 2 == 0 else \"LA\") if len(pp.buffer) == 0 else \"S\"\n",
        "                for pp in partial_parses]\n",
        "\n",
        "def test_dependencies(name, deps, ex_deps):\n",
        "    \"\"\"Tests the provided dependencies match the expected dependencies\"\"\"\n",
        "    deps = tuple(sorted(deps))\n",
        "    assert deps == ex_deps, \\\n",
        "        \"{:} test resulted in dependency list {:}, expected {:}\".format(name, deps, ex_deps)\n",
        "\n",
        "\n",
        "def test_minibatch_parse():\n",
        "    \"\"\"Simple tests for the minibatch_parse function\n",
        "    Warning: these are not exhaustive\n",
        "    \"\"\"\n",
        "\n",
        "    # Unidirectional arcs test\n",
        "    sentences = [[\"right\", \"arcs\", \"only\"],\n",
        "                 [\"right\", \"arcs\", \"only\", \"again\"],\n",
        "                 [\"left\", \"arcs\", \"only\"],\n",
        "                 [\"left\", \"arcs\", \"only\", \"again\"]]\n",
        "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
        "    test_dependencies(\"minibatch_parse\", deps[0],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[1],\n",
        "                      (('ROOT', 'right'), ('arcs', 'only'), ('only', 'again'), ('right', 'arcs')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[2],\n",
        "                      (('only', 'ROOT'), ('only', 'arcs'), ('only', 'left')))\n",
        "    test_dependencies(\"minibatch_parse\", deps[3],\n",
        "                      (('again', 'ROOT'), ('again', 'arcs'), ('again', 'left'), ('again', 'only')))\n",
        "\n",
        "    # Out-of-bound test\n",
        "    sentences = [[\"right\"]]\n",
        "    deps = minibatch_parse(sentences, DummyModel(), 2)\n",
        "    test_dependencies(\"minibatch_parse\", deps[0], (('ROOT', 'right'),))\n",
        "\n",
        "    # Mixed arcs test\n",
        "    sentences = [[\"this\", \"is\", \"interleaving\", \"dependency\", \"test\"]]\n",
        "    deps = minibatch_parse(sentences, DummyModel(mode=\"interleave\"), 1)\n",
        "    test_dependencies(\"minibatch_parse\", deps[0],\n",
        "                      (('ROOT', 'is'), ('dependency', 'interleaving'),\n",
        "                      ('dependency', 'test'), ('is', 'dependency'), ('is', 'this')))\n",
        "    print(\"minibatch_parse test passed!\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    args = sys.argv\n",
        "    if len(args) != 2:\n",
        "        raise Exception(\"You did not provide a valid keyword. Either provide 'part_c' or 'part_d', when executing this script\")\n",
        "    elif args[1] == \"part_c\":\n",
        "        test_parse_step()\n",
        "        test_parse()\n",
        "    elif args[1] == \"part_d\":\n",
        "        test_minibatch_parse()\n",
        "    else:\n",
        "        raise Exception(\"You did not provide a valid keyword. Either provide 'part_c' or 'part_d', when executing this script\")\n"
      ],
      "metadata": {
        "id": "erBAHTkAX5jF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2020-2021: Homework 3\n",
        "parser_model.py: Feed-Forward Neural Network for Dependency Parsing\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "Haoshen Hong <haoshen@stanford.edu>\n",
        "\"\"\"\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ParserModel(nn.Module):\n",
        "    \"\"\" Feedforward neural network with an embedding layer and two hidden layers.\n",
        "    The ParserModel will predict which transition should be applied to a\n",
        "    given partial parse configuration.\n",
        "\n",
        "    PyTorch Notes:\n",
        "        - Note that \"ParserModel\" is a subclass of the \"nn.Module\" class. In PyTorch all neural networks\n",
        "            are a subclass of this \"nn.Module\".\n",
        "        - The \"__init__\" method is where you define all the layers and parameters\n",
        "            (embedding layers, linear layers, dropout layers, etc.).\n",
        "        - \"__init__\" gets automatically called when you create a new instance of your class, e.g.\n",
        "            when you write \"m = ParserModel()\".\n",
        "        - Other methods of ParserModel can access variables that have \"self.\" prefix. Thus,\n",
        "            you should add the \"self.\" prefix layers, values, etc. that you want to utilize\n",
        "            in other ParserModel methods.\n",
        "        - For further documentation on \"nn.Module\" please see https://pytorch.org/docs/stable/nn.html.\n",
        "    \"\"\"\n",
        "    def __init__(self, embeddings, n_features=36,\n",
        "        hidden_size=200, n_classes=3, dropout_prob=0.5):\n",
        "        \"\"\" Initialize the parser model.\n",
        "\n",
        "        @param embeddings (ndarray): word embeddings (num_words, embedding_size)\n",
        "        @param n_features (int): number of input features\n",
        "        @param hidden_size (int): number of hidden units\n",
        "        @param n_classes (int): number of output classes\n",
        "        @param dropout_prob (float): dropout probability\n",
        "        \"\"\"\n",
        "        super(ParserModel, self).__init__()\n",
        "        self.n_features = n_features\n",
        "        self.n_classes = n_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "        self.embed_size = embeddings.shape[1]\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embeddings = nn.Parameter(torch.tensor(embeddings))\n",
        "\n",
        "        ### YOUR CODE HERE (~9-10 Lines)\n",
        "        ### TODO:\n",
        "        ###     1) Declare `self.embed_to_hidden_weight` and `self.embed_to_hidden_bias` as `nn.Parameter`.\n",
        "        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n",
        "        ###        with default parameters.\n",
        "        ###     2) Construct `self.dropout` layer.\n",
        "        ###     3) Declare `self.hidden_to_logits_weight` and `self.hidden_to_logits_bias` as `nn.Parameter`.\n",
        "        ###        Initialize weight with the `nn.init.xavier_uniform_` function and bias with `nn.init.uniform_`\n",
        "        ###        with default parameters.\n",
        "        ###\n",
        "        ### Note: Trainable variables are declared as `nn.Parameter` which is a commonly used API\n",
        "        ###       to include a tensor into a computational graph to support updating w.r.t its gradient.\n",
        "        ###       Here, we use Xavier Uniform Initialization for our Weight initialization.\n",
        "        ###       It has been shown empirically, that this provides better initial weights\n",
        "        ###       for training networks than random uniform initialization.\n",
        "        ###       For more details checkout this great blogpost:\n",
        "        ###             http://andyljones.tumblr.com/post/110998971763/an-explanation-of-xavier-initialization\n",
        "        ###\n",
        "        ### Please see the following docs for support:\n",
        "        ###     nn.Parameter: https://pytorch.org/docs/stable/nn.html#parameters\n",
        "        ###     Initialization: https://pytorch.org/docs/stable/nn.init.html\n",
        "        ###     Dropout: https://pytorch.org/docs/stable/nn.html#dropout-layers\n",
        "        ### \n",
        "        ### See the PDF for hints.\n",
        "\n",
        "        self.embed_to_hidden_weight = nn.Parameter()\n",
        "        self.embed_to_hidden_bias = nn.Parameter()\n",
        "        nn.init.xavier_uniform_(self.embed_to_hidden_weight)\n",
        "        nn.init.uniform_(self.embed_to_hidden_bias)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        self.hidden_to_logits_weight = nn.Parameter()\n",
        "        self.hidden_to_logits_bias = nn.Parameter()\n",
        "        nn.init.xavier_uniform_(self.hidden_to_logits_weight)\n",
        "        nn.init.uniform_(self.hidden_to_logits_bias)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "\n",
        "    def embedding_lookup(self, w):\n",
        "        \"\"\" Utilize `w` to select embeddings from embedding matrix `self.embeddings`\n",
        "            @param w (Tensor): input tensor of word indices (batch_size, n_features)\n",
        "\n",
        "            @return x (Tensor): tensor of embeddings for words represented in w\n",
        "                                (batch_size, n_features * embed_size)\n",
        "        \"\"\"\n",
        "\n",
        "        ### YOUR CODE HERE (~1-4 Lines)\n",
        "        ### TODO:\n",
        "        ###     1) For each index `i` in `w`, select `i`th vector from self.embeddings\n",
        "        ###     2) Reshape the tensor using `view` function if necessary\n",
        "        ###\n",
        "        ### Note: All embedding vectors are stacked and stored as a matrix. The model receives\n",
        "        ###       a list of indices representing a sequence of words, then it calls this lookup\n",
        "        ###       function to map indices to sequence of embeddings.\n",
        "        ###\n",
        "        ###       This problem aims to test your understanding of embedding lookup,\n",
        "        ###       so DO NOT use any high level API like nn.Embedding\n",
        "        ###       (we are asking you to implement that!). Pay attention to tensor shapes\n",
        "        ###       and reshape if necessary. Make sure you know each tensor's shape before you run the code!\n",
        "        ###\n",
        "        ### Pytorch has some useful APIs for you, and you can use either one\n",
        "        ### in this problem (except nn.Embedding). These docs might be helpful:\n",
        "        ###     Index select: https://pytorch.org/docs/stable/torch.html#torch.index_select\n",
        "        ###     Gather: https://pytorch.org/docs/stable/torch.html#torch.gather\n",
        "        ###     View: https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view\n",
        "        ###     Flatten: https://pytorch.org/docs/stable/generated/torch.flatten.html\n",
        "\n",
        "        x = self.pretrained_embeddings(t) # (batch_size, n_features, embed_size)\n",
        "        x = x.view(t.shape[0], -1)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self, w):\n",
        "        \"\"\" Run the model forward.\n",
        "\n",
        "            Note that we will not apply the softmax function here because it is included in the loss function nn.CrossEntropyLoss\n",
        "\n",
        "            PyTorch Notes:\n",
        "                - Every nn.Module object (PyTorch model) has a `forward` function.\n",
        "                - When you apply your nn.Module to an input tensor `w` this function is applied to the tensor.\n",
        "                    For example, if you created an instance of your ParserModel and applied it to some `w` as follows,\n",
        "                    the `forward` function would called on `w` and the result would be stored in the `output` variable:\n",
        "                        model = ParserModel()\n",
        "                        output = model(w) # this calls the forward function\n",
        "                - For more details checkout: https://pytorch.org/docs/stable/nn.html#torch.nn.Module.forward\n",
        "\n",
        "        @param w (Tensor): input tensor of tokens (batch_size, n_features)\n",
        "\n",
        "        @return logits (Tensor): tensor of predictions (output after applying the layers of the network)\n",
        "                                 without applying softmax (batch_size, n_classes)\n",
        "        \"\"\"\n",
        "        ### YOUR CODE HERE (~3-5 lines)\n",
        "        ### TODO:\n",
        "        ###     Complete the forward computation as described in write-up. In addition, include a dropout layer\n",
        "        ###     as declared in `__init__` after ReLU function.\n",
        "        ###\n",
        "        ### Note: We do not apply the softmax to the logits here, because\n",
        "        ### the loss function (torch.nn.CrossEntropyLoss) applies it more efficiently.\n",
        "        ###\n",
        "        ### Please see the following docs for support:\n",
        "        ###     Matrix product: https://pytorch.org/docs/stable/torch.html#torch.matmul\n",
        "        ###     ReLU: https://pytorch.org/docs/stable/nn.html?highlight=relu#torch.nn.functional.relu\n",
        "        embeddings = self.embedding_lookup(t) # (batch_size, n_features * embedding_size)\n",
        "        embeddings = self.embed_to_hidden(embeddings) # (batch_size, hidden_size)\n",
        "        h = F.relu_(embeddings) # (batch_size, hidden_size)\n",
        "        h_dropout = self.dropout(h) # (batch_size, hidden_size)\n",
        "        logits = self.hidden_to_logits(h_dropout) # (batch_size, num_classes)\n",
        "\n",
        "        ### END YOUR CODE\n",
        "        return logits\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='Simple sanity check for parser_model.py')\n",
        "    parser.add_argument('-e', '--embedding', action='store_true', help='sanity check for embeding_lookup function')\n",
        "    parser.add_argument('-f', '--forward', action='store_true', help='sanity check for forward function')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    embeddings = np.zeros((100, 30), dtype=np.float32)\n",
        "    model = ParserModel(embeddings)\n",
        "\n",
        "    def check_embedding():\n",
        "        inds = torch.randint(0, 100, (4, 36), dtype=torch.long)\n",
        "        selected = model.embedding_lookup(inds)\n",
        "        assert np.all(selected.data.numpy() == 0), \"The result of embedding lookup: \" \\\n",
        "                                      + repr(selected) + \" contains non-zero elements.\"\n",
        "\n",
        "    def check_forward():\n",
        "        inputs =torch.randint(0, 100, (4, 36), dtype=torch.long)\n",
        "        out = model(inputs)\n",
        "        expected_out_shape = (4, 3)\n",
        "        assert out.shape == expected_out_shape, \"The result shape of forward is: \" + repr(out.shape) + \\\n",
        "                                                \" which doesn't match expected \" + repr(expected_out_shape)\n",
        "\n",
        "    if args.embedding:\n",
        "        check_embedding()\n",
        "        print(\"Embedding_lookup sanity check passes!\")\n",
        "\n",
        "    if args.forward:\n",
        "        check_forward()\n",
        "        print(\"Forward sanity check passes!\")"
      ],
      "metadata": {
        "id": "CVne9GxBcIaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "CS224N 2020-2021: Homework 3\n",
        "run.py: Run the dependency parser.\n",
        "Sahil Chopra <schopra8@stanford.edu>\n",
        "Haoshen Hong <haoshen@stanford.edu>\n",
        "\"\"\"\n",
        "from datetime import datetime\n",
        "import os\n",
        "import pickle\n",
        "import math\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "from torch import nn, optim\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "from parser_model import ParserModel\n",
        "from utils.parser_utils import minibatches, load_and_preprocess_data, AverageMeter\n",
        "\n",
        "parser = argparse.ArgumentParser(description='Train neural dependency parser in pytorch')\n",
        "parser.add_argument('-d', '--debug', action='store_true', help='whether to enter debug mode')\n",
        "args = parser.parse_args()\n",
        "\n",
        "# -----------------\n",
        "# Primary Functions\n",
        "# -----------------\n",
        "def train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005):\n",
        "    \"\"\" Train the neural dependency parser.\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param output_path (str): Path to which model weights and results are written.\n",
        "    @param batch_size (int): Number of examples in a single batch\n",
        "    @param n_epochs (int): Number of training epochs\n",
        "    @param lr (float): Learning rate\n",
        "    \"\"\"\n",
        "    best_dev_UAS = 0\n",
        "\n",
        "\n",
        "    ### YOUR CODE HERE (~2-7 lines)\n",
        "    ### TODO:\n",
        "    ###      1) Construct Adam Optimizer in variable `optimizer`\n",
        "    ###      2) Construct the Cross Entropy Loss Function in variable `loss_func` with `mean`\n",
        "    ###         reduction (default)\n",
        "    ###\n",
        "    ### Hint: Use `parser.model.parameters()` to pass optimizer\n",
        "    ###       necessary parameters to tune.\n",
        "    ### Please see the following docs for support:\n",
        "    ###     Adam Optimizer: https://pytorch.org/docs/stable/optim.html\n",
        "    ###     Cross Entropy Loss: https://pytorch.org/docs/stable/nn.html#crossentropyloss\n",
        "    optimizer = torch.optim.Adam(parser.model.parameters())\n",
        "    loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    ### END YOUR CODE\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        print(\"Epoch {:} out of {:}\".format(epoch + 1, n_epochs))\n",
        "        dev_UAS = train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size)\n",
        "        if dev_UAS > best_dev_UAS:\n",
        "            best_dev_UAS = dev_UAS\n",
        "            print(\"New best dev UAS! Saving model.\")\n",
        "            torch.save(parser.model.state_dict(), output_path)\n",
        "        print(\"\")\n",
        "\n",
        "\n",
        "def train_for_epoch(parser, train_data, dev_data, optimizer, loss_func, batch_size):\n",
        "    \"\"\" Train the neural dependency parser for single epoch.\n",
        "\n",
        "    Note: In PyTorch we can signify train versus test and automatically have\n",
        "    the Dropout Layer applied and removed, accordingly, by specifying\n",
        "    whether we are training, `model.train()`, or evaluating, `model.eval()`\n",
        "\n",
        "    @param parser (Parser): Neural Dependency Parser\n",
        "    @param train_data ():\n",
        "    @param dev_data ():\n",
        "    @param optimizer (nn.Optimizer): Adam Optimizer\n",
        "    @param loss_func (nn.CrossEntropyLoss): Cross Entropy Loss Function\n",
        "    @param batch_size (int): batch size\n",
        "\n",
        "    @return dev_UAS (float): Unlabeled Attachment Score (UAS) for dev data\n",
        "    \"\"\"\n",
        "    parser.model.train() # Places model in \"train\" mode, i.e. apply dropout layer\n",
        "    n_minibatches = math.ceil(len(train_data) / batch_size)\n",
        "    loss_meter = AverageMeter()\n",
        "\n",
        "    with tqdm(total=(n_minibatches)) as prog:\n",
        "        for i, (train_x, train_y) in enumerate(minibatches(train_data, batch_size)):\n",
        "            optimizer.zero_grad()   # remove any baggage in the optimizer\n",
        "            loss = 0. # store loss for this batch here\n",
        "            train_x = torch.from_numpy(train_x).long()\n",
        "            train_y = torch.from_numpy(train_y.nonzero()[1]).long()\n",
        "\n",
        "            ### YOUR CODE HERE (~4-10 lines)\n",
        "            ### TODO:\n",
        "            ###      1) Run train_x forward through model to produce `logits`\n",
        "            ###      2) Use the `loss_func` parameter to apply the PyTorch CrossEntropyLoss function.\n",
        "            ###         This will take `logits` and `train_y` as inputs. It will output the CrossEntropyLoss\n",
        "            ###         between softmax(`logits`) and `train_y`. Remember that softmax(`logits`)\n",
        "            ###         are the predictions (y^ from the PDF).\n",
        "            ###      3) Backprop losses\n",
        "            ###      4) Take step with the optimizer\n",
        "            ### Please see the following docs for support:\n",
        "            ###     Optimizer Step: https://pytorch.org/docs/stable/optim.html#optimizer-step\n",
        "\n",
        "            logits = parser.model.forward(train_x)\n",
        "            loss = loss_func(logits, train_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            ### END YOUR CODE\n",
        "            prog.update(1)\n",
        "            loss_meter.update(loss.item())\n",
        "\n",
        "    print (\"Average Train Loss: {}\".format(loss_meter.avg))\n",
        "\n",
        "    print(\"Evaluating on dev set\",)\n",
        "    parser.model.eval() # Places model in \"eval\" mode, i.e. don't apply dropout layer\n",
        "    dev_UAS, _ = parser.parse(dev_data)\n",
        "    print(\"- dev UAS: {:.2f}\".format(dev_UAS * 100.0))\n",
        "    return dev_UAS\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    debug = args.debug\n",
        "\n",
        "    assert (torch.__version__.split(\".\") >= [\"1\", \"0\", \"0\"]), \"Please install torch version >= 1.0.0\"\n",
        "\n",
        "    print(80 * \"=\")\n",
        "    print(\"INITIALIZING\")\n",
        "    print(80 * \"=\")\n",
        "    parser, embeddings, train_data, dev_data, test_data = load_and_preprocess_data(debug)\n",
        "\n",
        "    start = time.time()\n",
        "    model = ParserModel(embeddings)\n",
        "    parser.model = model\n",
        "    print(\"took {:.2f} seconds\\n\".format(time.time() - start))\n",
        "\n",
        "    print(80 * \"=\")\n",
        "    print(\"TRAINING\")\n",
        "    print(80 * \"=\")\n",
        "    output_dir = \"results/{:%Y%m%d_%H%M%S}/\".format(datetime.now())\n",
        "    output_path = output_dir + \"model.weights\"\n",
        "\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    train(parser, train_data, dev_data, output_path, batch_size=1024, n_epochs=10, lr=0.0005)\n",
        "\n",
        "    if not debug:\n",
        "        print(80 * \"=\")\n",
        "        print(\"TESTING\")\n",
        "        print(80 * \"=\")\n",
        "        print(\"Restoring the best model weights found on the dev set\")\n",
        "        parser.model.load_state_dict(torch.load(output_path))\n",
        "        print(\"Final evaluation on test set\",)\n",
        "        parser.model.eval()\n",
        "        UAS, dependencies = parser.parse(test_data)\n",
        "        print(\"- test UAS: {:.2f}\".format(UAS * 100.0))\n",
        "        print(\"Done!\")\n"
      ],
      "metadata": {
        "id": "g9prCWbxZRBe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}